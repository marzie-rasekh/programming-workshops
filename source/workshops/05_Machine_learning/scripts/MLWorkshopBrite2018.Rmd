---
title: "MLWorkshopBrite2018"
author: "Dakota Hawkins"
date: "July 14, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction

In this workshop, we'll use the GSE53987 dataset to explore and gain experience
with common machine learning techniques. The dataset contains gene expression
profiles is post-mortem human brain samples from subject with schizophrenia,
bioplor disorder, and major depressive disorder. There are also a few metadata 
features included, such as sex, race, and age. This workshop will explore the relationship between metadata factors and gene expression patterns within tissue types and mental health status.

## Set Up

First, we need to load the dataset and a few helper functions we've written
into our `R` environments. From the top of the Github repository, the data 
can be found at `/source/workshops/05_Machine_learning/data/combined_data.csv`.
```{r}
#combined_data <- read.csv('../data/GSE53987_combined.csv', row.names=1)
```


Next, we need to load in a few helper functions we've written to make plotting
and data exploration easier.

```{r}
library(ggplot2)
library(reshape2)
library(caret)
library(e1071)
#' create_boxplot
#'
#' Create grouped violin/boxplots for a specified feature.
#'
#' @param dataframe (data.frame): data.frame containing data to plot.
#' @param y_column (string): column name to plot along the y-axis.
#' @param group_column (string): column name for grouping variable.
#' @param facet (string, optional): second grouping variable. Will create
#'     distinct boxplots for each unique value within the column. Boxplots are
#'     stacked in rows. Default is '', and no facet wrapping is applied.
#'
#' @return (gg.ggplot): ggplot object of boxplots
#' @export
#'
#' @examples
#'
#' # plot 'Pmi' accross disease states
#' create_boxplots(combined_data, 'Pmi', 'Disease.state')
#'
#' # plot 'Pmi' accross disease states and tissue types
#' create_boxplots(combined_data, 'Pmi', 'Disease.state', 'Tissue')
create_boxplot <- function(dataframe, y_column, group_column, facet='') {
  boxplots <- ggplot(dataframe, aes_string(x=group_column, y=y_column)) +
              geom_violin(aes_string(fill=group_column), trim=FALSE) +
              geom_boxplot(width=0.1) +
              theme_minimal() +
              theme(axis.text.x = element_text(angle=45, hjust=1))
  if (facet != '' && facet %in% colnames(dataframe)) {
    boxplots <- boxplots + facet_grid(reformulate('.', facet))
  }
  return(boxplots)
}

#' create_scatterplot()
#'
#' Create a scatterplot!
#'
#' @param dataframe (data.frame): data.frame containing data to plot.
#' @param x_column (string): column name for variable to plot along the x-axis.
#' @param y_column (string): column name for variable to plot along the y-axis.
#' @param color_column (string, optional): column name for variable to dictate
#'     dot color. Default is '' with no coloring.
#'
#' @return (gg.ggplot): ggplot object of scatterplot
#' @export
#'
#' @examples
#'
#' # plot 'A1CF' and 'A2M' expression
#' create_scatterplot(combined_data, 'A1CF', 'A2M')
#'
#' # PLOT 'A1CF' and 'A2M' expression with 'Tissue' coloring
#' create_scatterplot(combined_data, 'A1CF', 'A2M', 'Tissue')
create_scatterplot <- function(dataframe, x_column, y_column, color_column='') {
  scatter <- ggplot(dataframe, aes_string(x=x_column, y=y_column))
  if (color_column != '' && color_column %in% colnames(dataframe)) {
    scatter <- scatter + geom_point(aes_string(color=color_column))
  } else {
    scatter <- scatter + geom_point()
  }
  return(scatter)
}

#' create_histogram()
#'
#' Create a histogram!
#'
#' @param dataframe (data.frame): dataframe containing data to plot.
#' @param column (string): column name of variable to plot.
#' @param facet (string, optional): categorical variable to separate data on.
#'     Default is `''` with no separation.
#' @param bins (int, optional): number of bins in histogram. Default is 30.
#'
#' @return (gg.ggplot): ggplot object of histograms.
#' @export
#'
#' @examples

create_histogram <- function(dataframe, column, facet='', bins=30) {
  hist <- ggplot(dataframe, aes_string(column)) +
          geom_histogram(bins=bins)
  if (facet != '' && facet %in% colnames(dataframe)) {
    hist <- hist + facet_grid(reformulate('.', facet))
  }
  return(hist)
}
```
## Exploration 1: Features

Check all the features. Which features are numeric, which are categorical? Understanding the nature of your data is a very important and necessary first step before proceeding with any analysis.

### Distributions/Histograms
What type of distributions exist within the features? Is Gender a balanced feature (roughly equal representation between both men and women)? Are numerical values normally distributed? Explore numerical distributions by plotting histograms for Age, an Age + Gender histogram, and one of your favorite genes found in the dataset. Discuss your findings below your code.

```{r}

```

### Factor Dependence/Boxplots
Some features display factor dependent values. That is, whether a subject is a male or a female might effect the expression patterns of a given gene. Explore factor and feature relationships by creating boxplots for three different features grouped by Tissue, Disease.status, and combining the two with the `facet` parameter in `create_boxplot()`. Discuss your finding below your code. If we want to predict factor/group membership (e.g. tissue of origin), what types of boxplots would we expect to see from a predictive feature?

```{r}

```


## Exploration 2: Principal Component Analysis

Principal Component Analysis (PCA) is a commonly used technique to create linearly uncorrelated features from a set of possibly correlated features. The procedure is done in such a way that the first feature produced by PCA, the first principal component -- PC1, explains the largest amount of variability possible. In this way, PCA is a dimension reduction technique, as the first few principal components often explain upwards of 90% of the variability found within a dataset. You can perform PCA in R using the `prcomp` function. Give your R terminal that fatty `?prcomp` to look up how to use the function. It is important to note that if we're planning on predicting anything using the principal components, such as tissue type or Disease.status, those features should *not* be included in the matrix that is passed to `prcomp`. Before performing PCA, create a new data.table containing only explanatory values (i.e. the features we want to use to predict class membership).

### Variation Explanation

Explore how much variation is explained by the principal components.How much variation is explained by the first two principal components? How many principal components are required to explain 75%, 85%, 90%, 95%, and 99% of the variation within our dataset?


### Separation in the Principal Components

Because principal components contain so much information, they are often used to separate samples from different factor groups. Visually explore this separation using the `create_histogram()` function to plot the first two principal components and color samples according to Tissue and Disease.status. What effect does plotting the third principal component have on sample separation?


### Tissue Dependent Separation

It is possible samples from one tissue are more predictive of Disease.status than samples from another tissue. Subset the dataset into three disjoint datasets by Tissue. Run PCA on all three of these datasets, plot the first two principal components, and color the dots according to Disease.status. Does there appear to be a meaningful difference in the separation between disease classes between the three different datasets?

## Exploration 3: Feature Selection

Feature selection is a commonly performed step in statistics/machine learning to distinguish the most informative variable to use in model creation. There are several different ways to perform feature selection, and many of these can be application specific. In this workshop we'll explain four possible avenues for feature selection in gene expression data analysis: 1) selecting the $x$ most variable features, 2) selecting features with the $x$ largest correlations with principal components, 3) selecting the top $x$ differentially expressed genes between defined groups, and 4) calculating Fisher's Criterion for features between groups. Use one of the below methods to select the most informative features.

### 1. Most variable features

Calculating the most variable features in a dataset is relatively straight forward using the `var` or `cov` function in R. Ranking the values in the matrix diagnol will produce the most variable genes.

### 2. Highest correlations with principal components

Correlation explains how much variation in a variable $y$ is explained by the variation in variable $x$. Knowing that principal components maximize the variance found within a dataset, finding how much each feature correlates with each prinicipal component thus gives us a measure for how informative various features are. Because different principal components likely explain different parts of the data, it may be important to select variables that are highly correlated with different components.

### 3. Differential Expression

If we have pre-defined groups that we know we're interested in predicting, such a `Disease.status` or `Tissue`, we can find genes that are differentially expressed between the groups, and then subset our dataset down to the most differentially expressed. When considering ranking differentially expressed genes, it is important to consider *both* statistical significance (i.e. a p-value) as well as the biological difference (i.e. average log-fold change between groups).

### 4. Largest Fisher's Criterion

Fisher's Criterion, otherwise known as Fisher's linear discriminant, is used in linear discrimant analysis for classification. However, given predefined groups $i$ and $j$, we can calculate the statistic to find the informative features:

$F.C. = \frac{(\mu_i - \mu_j)}{\sigma_i^2 + \sigma_j^2}$

## Exploration 4: Highly Related Features

With datasets of any size, it is unlikely our features are entirely independent. Instead, there is likely redundant information contained in multiple features. While this is not a problem for some algorithms, with standard algorithms such as multi-variate linear regression, co-linearity between variables can lead to a highly overfit model. You might think of this as "double counting". Even if a specific algorithm may be able to deal with related variables, removing non-informative features can still help with overfitting and reducing run time. To visualize highly correlated features, first calculate the correlation matrix between your numerical variables, and visualize the relationships by calling the `correlation_plot()` function. Which features are highly related to each other? How do these relationships change if we subset our dataset by tissue type? If you would like, feel free to remove highly correlated variables. However, you'll need to decide how to determine which features to keep among those that are highly related.

## Exploration 5: Unsupervised Learning - Clustering

Unsupervised learning can be thought of as applying an algorithm to a dataset in order to discover latent structure that exists between samples. We've already been exposed to some of these algorithms via PCA. However, one of the most common techniques in machine learning, and especially bioinformatics, is hierarchical clustering. Hierarchical clustering builds a dendogram/tree relating similar samples together. While hierarchical clustering itself doesn't produce distinct clusters, cutting the dendogram at different three heights does. However, even without producing distinct clusters the technique is still useful. The `heatmap()` function in `R` clusters both samples and features to group highly related features along with highly related samples. Use the `heatmap()` function in R to visualize the previously created correlation matrix. Is it easier to parse which features are highly related? Heatmaps are also commonly used to visualize gene expression patterns. Isolating only the gene expression values from your dataset, use the `heatmap()` function to explore sample and gene clusters. Do any genes form distinct clusters? What about samples? 


## Exploration 6: Supervise Learning

Supervise learning is a technique to teach an algorithm to distinguish between previously labelled groups, such as `Tissue`, `Gender`, or `Disease.status`. However, all supervised methods require data to learn how to differentiate between classes. Therefore, it is necessary to separate data into test/train sets. The training set is used to train the model, while the test set is used to evaluate performance. Cross-validation, a method of partitioning the data into disjoint subsets and continually re-training and re-testing with different partition combinations, is often used to evaluate models. In `R`, you can use the `createFolds` function in the `caret` package to easily partition your data. Load/install the library, and `?createFolds` for more information. In this section, we will build various classifiers to predict different classes from our data. You should evaluate your models' performances using the AUC in ROC space -- at least for binary classes -- along with confusion matrices.

### Logistic Regression

Logistic regression is the canonical example of supervised learning. You can perform logistic regression in `R` using the `glm` function uisng the following syntax: `glm(<label_variable> ~ <predictor_variable>, data=<dataframe>, family=binomial)`. Multivariate logistic regression can be performed by adding variables to the formula provided as the first argument (e.g. `glm(<label_variable> ~ <predictor_1> + <predictor_2>, ...)`). Using the below code as an example, try to predict a class label of your choice using logistic regression.

```{r}
  y <- dataframe$Gender
  folds <- caret::createFolds(y, k=10)
  
  # bind outputted list auc scores to dataframe 
  # for future averaging
  metrics <- lapply(1:length(folds), function(i) {
    test_idx <- folds[[i]]  # get current test partition
    test <- dataframe[test_idx, ]
    train_idx <- unlist(folds[c(1:length(folds))[-i]])  # get train partions
    train <- dataframe[train_idx, ]
    
    # create model --> formula Gender ~ A2m is predicting Gender from 
    # A2m expression | Gender ~ . would predict Gender using all features
    model <- glm(Gender ~ A2M, data=train, family=binomial)
    
    test_pred <- predict(model, test)  # predict test labels from data
    pred_metrics <- ROCR::prediction(test_pred, test$Gender)
    roc_values <- ROCR::performance(pred_metrics, measure = 'tpr', x.measure='fpr')
    auc <- ROCR::performance(pred_metrics, measure='auc')
    auc <- auc@y.values[[1]]
    return(list('fpr' = roc_values@x.values[[1]], 
                'tpr' = roc_values@y.values[[1]], 'auc'=auc))
  })
  
  # Concatenate metrics from different runs
  fpr <- c()
  tpr <- c()
  fold <- c()
  roc <- c()
  for (i in 1:length(metrics)) {
    fpr <- c(fpr, metrics[[i]]$fpr)
    tpr <- c(tpr, metrics[[i]]$tpr)
    roc <- c(roc, metrics[[i]]$auc)
    fold <- c(fold, seq(i, i, length.out=length(metrics[[i]]$fpr)))
  }
  
  # create data frames for ggplotting
  roc_df <- data.frame('fpr'=fpr, 'tpr'=tpr,'fold'=fold)
  roc_df$fold <- as.factor(roc_df$fold)
  linspace <- seq(from=0, to=1, length.out=1000)
  average_roc <- approx(fpr, tpr, xout=linspace)
  average_df <- data.frame('fpr'=average_roc$x, 'tpr'=average_roc$y)
  
  # plot roc curve + auc 
  mean_auc <- round(mean(auc), 3)
  ggplot(data=roc_df, aes(x=fpr, y=tpr)) +
    geom_line(aes(color=fold), alpha=0.75) + 
    geom_line(data=average_df, color='black') + 
    ggtitle("ROC Curve", subtitle=paste0('AUC = ',
                                           as.character(mean_auc)))

```